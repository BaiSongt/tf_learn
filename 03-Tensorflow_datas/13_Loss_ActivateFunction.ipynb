{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f014a253",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "source": [
    "### TensorFlow 中 Loss 计算的理论与操作\n",
    "\n",
    "在深度学习中，**Loss（损失）** 是衡量模型预测结果与真实值之间差异的指标。优化模型的目标是通过调整参数使损失函数的值最小化，从而提高模型的预测能力。TensorFlow 提供了多种损失函数以及自定义损失的能力，适用于不同的任务场景。\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 损失函数的基本理论\n",
    "\n",
    "损失函数可以分为以下几类：\n",
    "\n",
    "1. **回归任务的损失函数**：\n",
    "  - 均方误差（Mean Squared Error, MSE）：\n",
    "    $$\n",
    "    \\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "    $$\n",
    "    其中，$y_i$ 是真实值，$\\hat{y}_i$ 是预测值。\n",
    "\n",
    "  - 平均绝对误差（Mean Absolute Error, MAE）：\n",
    "    $$\n",
    "    \\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|\n",
    "    $$\n",
    "\n",
    "2. **分类任务的损失函数**：\n",
    "  - 交叉熵损失（Cross-Entropy Loss）：\n",
    "    对于二分类任务：\n",
    "    $$\n",
    "    \\text{Binary Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "    $$\n",
    "    对于多分类任务：\n",
    "    $$\n",
    "    \\text{Categorical Cross-Entropy} = -\\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log(\\hat{y}_{ij})\n",
    "    $$\n",
    "    其中，$k$ 是类别数，$y_{ij}$ 是真实标签的 one-hot 编码，$\\hat{y}_{ij}$ 是预测概率。\n",
    "\n",
    "3. **自定义损失函数**：\n",
    "  - 可以根据具体需求定义损失函数，例如加权损失、对抗损失等。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. TensorFlow 中的损失函数操作\n",
    "\n",
    "TensorFlow 提供了 `tf.keras.losses` 模块，其中包含了常用的损失函数。以下是一些常见的操作示例：\n",
    "\n",
    "##### 2.1 使用内置损失函数\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 示例数据\n",
    "y_true = [[0., 1.], [0., 0.]]\n",
    "y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
    "\n",
    "# 使用 Binary Cross-Entropy\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "loss = bce(y_true, y_pred)\n",
    "print(\"Binary Cross-Entropy Loss:\", loss.numpy())\n",
    "\n",
    "# 使用 Mean Squared Error\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "loss = mse(y_true, y_pred)\n",
    "print(\"Mean Squared Error Loss:\", loss.numpy())\n",
    "```\n",
    "\n",
    "##### 2.2 自定义损失函数\n",
    "\n",
    "可以通过定义一个函数来实现自定义损失：\n",
    "\n",
    "```python\n",
    "# 自定义损失函数：加权均方误差\n",
    "def custom_loss(y_true, y_pred):\n",
    "   weights = tf.constant([0.3, 0.7])  # 自定义权重\n",
    "   squared_diff = tf.square(y_true - y_pred)\n",
    "   weighted_loss = tf.reduce_mean(weights * squared_diff)\n",
    "   return weighted_loss\n",
    "\n",
    "# 使用自定义损失\n",
    "y_true = tf.constant([1.0, 0.0])\n",
    "y_pred = tf.constant([0.8, 0.2])\n",
    "loss = custom_loss(y_true, y_pred)\n",
    "print(\"Custom Loss:\", loss.numpy())\n",
    "```\n",
    "\n",
    "##### 2.3 在模型中使用损失函数\n",
    "\n",
    "在 TensorFlow 的 `Model` 中，可以直接指定损失函数：\n",
    "\n",
    "```python\n",
    "# 构建简单模型\n",
    "model = tf.keras.Sequential([\n",
    "   tf.keras.layers.Dense(10, activation='relu'),\n",
    "   tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 编译模型时指定损失函数\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 示例数据\n",
    "import numpy as np\n",
    "X = np.random.random((100, 20))\n",
    "y = np.random.randint(2, size=(100, 1))\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X, y, epochs=5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. 注意事项\n",
    "\n",
    "1. **损失函数的选择**：\n",
    "  - 回归任务通常使用 MSE 或 MAE。\n",
    "  - 分类任务通常使用交叉熵损失。\n",
    "  - 根据任务需求选择合适的损失函数。\n",
    "\n",
    "2. **数值稳定性**：\n",
    "  - 在计算交叉熵损失时，可能会出现数值溢出问题。TensorFlow 内置的损失函数已经处理了这些问题。\n",
    "\n",
    "3. **自定义损失的梯度计算**：\n",
    "  - TensorFlow 会自动计算自定义损失的梯度，因此只需定义损失函数的计算逻辑。\n",
    "\n",
    "---\n",
    "\n",
    "通过以上内容，你应该对 TensorFlow 中损失函数的理论和操作有了初步了解。建议在实际项目中多实践，熟悉不同损失函数的使用场景和效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ea692",
   "metadata": {},
   "source": [
    "### 交叉熵损失（Cross-Entropy Loss）详解\n",
    "\n",
    "交叉熵损失是深度学习中最常用的损失函数之一，特别是在分类任务中。它衡量的是模型预测的概率分布与真实分布之间的差异。交叉熵损失的目标是最小化这种差异，从而提高模型的预测准确性。\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 交叉熵的定义\n",
    "\n",
    "交叉熵的数学公式如下：\n",
    "\n",
    "对于二分类任务：\n",
    "$$\n",
    "\\text{Binary Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^n \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "对于多分类任务：\n",
    "$$\n",
    "\\text{Categorical Cross-Entropy} = -\\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^k y_{ij} \\log(\\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $n$ 是样本数量。\n",
    "- $k$ 是类别数量。\n",
    "- $y_i$ 是真实标签（对于二分类任务，$y_i \\in \\{0, 1\\}$）。\n",
    "- $\\hat{y}_i$ 是模型预测的概率（对于二分类任务，$\\hat{y}_i \\in [0, 1]$）。\n",
    "- $y_{ij}$ 是第 $i$ 个样本的真实标签的 one-hot 编码。\n",
    "- $\\hat{y}_{ij}$ 是第 $i$ 个样本属于第 $j$ 类的预测概率。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 交叉熵的直观理解\n",
    "\n",
    "交叉熵损失的核心思想是：**如果模型的预测概率与真实标签的分布越接近，损失值就越小**。\n",
    "\n",
    "- **二分类任务**：\n",
    "  - 如果真实标签 $y_i = 1$，交叉熵损失只考虑 $-\\log(\\hat{y}_i)$，即预测概率 $\\hat{y}_i$ 越接近 1，损失越小。\n",
    "  - 如果真实标签 $y_i = 0$，交叉熵损失只考虑 $-\\log(1 - \\hat{y}_i)$，即预测概率 $\\hat{y}_i$ 越接近 0，损失越小。\n",
    "\n",
    "- **多分类任务**：\n",
    "  - 交叉熵损失会关注真实类别对应的预测概率 $\\hat{y}_{ij}$，预测概率越接近 1，损失越小。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. TensorFlow 中的实现\n",
    "\n",
    "在 TensorFlow 中，可以使用 `tf.keras.losses.BinaryCrossentropy` 和 `tf.keras.losses.CategoricalCrossentropy` 来计算交叉熵损失。\n",
    "\n",
    "##### 二分类任务示例\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 示例数据\n",
    "y_true = [[1.], [0.], [1.]]  # 真实标签\n",
    "y_pred = [[0.9], [0.2], [0.7]]  # 模型预测概率\n",
    "\n",
    "# 二分类交叉熵\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "loss = bce(y_true, y_pred)\n",
    "print(\"Binary Cross-Entropy Loss:\", loss.numpy())\n",
    "```\n",
    "\n",
    "##### 多分类任务示例\n",
    "\n",
    "```python\n",
    "# 示例数据\n",
    "y_true = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]  # 真实标签（one-hot 编码）\n",
    "y_pred = [[0.8, 0.1, 0.1], [0.2, 0.7, 0.1], [0.1, 0.3, 0.6]]  # 模型预测概率\n",
    "\n",
    "# 多分类交叉熵\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "loss = cce(y_true, y_pred)\n",
    "print(\"Categorical Cross-Entropy Loss:\", loss.numpy())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 数值稳定性\n",
    "\n",
    "在实际计算中，直接使用 $\\log(\\hat{y})$ 可能会导致数值溢出或不稳定问题。为了解决这个问题，TensorFlow 的交叉熵实现中通常会对 $\\hat{y}$ 加入一个小的平滑值 $\\epsilon$，例如：\n",
    "$$\n",
    "\\hat{y} = \\max(\\hat{y}, \\epsilon)\n",
    "$$\n",
    "这样可以避免 $\\log(0)$ 的情况。\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 使用场景\n",
    "\n",
    "- **二分类任务**：使用 `BinaryCrossentropy`。\n",
    "- **多分类任务**：\n",
    "  - 如果标签是 one-hot 编码，使用 `CategoricalCrossentropy`。\n",
    "  - 如果标签是整数编码，使用 `SparseCategoricalCrossentropy`。\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. 优势与注意事项\n",
    "\n",
    "- **优势**：\n",
    "  - 交叉熵损失直接作用于概率分布，能够有效指导模型优化。\n",
    "  - 在分类任务中表现优异，尤其是当类别不平衡时。\n",
    "\n",
    "- **注意事项**：\n",
    "  - 确保模型的输出经过 softmax 或 sigmoid 激活函数，以保证预测值是有效的概率分布。\n",
    "  - 对于多分类任务，标签格式（one-hot 或整数编码）需要与损失函数匹配。\n",
    "\n",
    "---\n",
    "\n",
    "通过交叉熵损失的优化，模型能够逐步调整参数，使预测概率分布更接近真实分布，从而提高分类性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b439a6b0",
   "metadata": {},
   "source": [
    "### TensorFlow 中激活函数计算的理论与详解\n",
    "\n",
    "在深度学习中，**激活函数（Activation Function）** 是神经网络的重要组成部分。它的作用是引入非线性，使得神经网络能够拟合复杂的函数关系。TensorFlow 提供了多种常用的激活函数，适用于不同的任务场景。\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 激活函数的基本理论\n",
    "\n",
    "激活函数的主要作用包括：\n",
    "1. **非线性变换**：将线性模型扩展为非线性模型，增强网络的表达能力。\n",
    "2. **特征提取**：通过激活函数的变换，提取输入数据的特征。\n",
    "3. **梯度传播**：激活函数的导数影响梯度的传播和模型的训练效果。\n",
    "\n",
    "激活函数可以分为以下几类：\n",
    "- **线性激活函数**：如恒等函数。\n",
    "- **非线性激活函数**：如 Sigmoid、ReLU、Tanh 等。\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 常见激活函数详解\n",
    "\n",
    "##### 2.1 Sigmoid 函数\n",
    "\n",
    "Sigmoid 函数的公式为：\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **特点**：\n",
    "  - 输出范围为 $(0, 1)$。\n",
    "  - 常用于二分类任务的输出层。\n",
    "- **优点**：\n",
    "  - 将输入值映射到 $(0, 1)$，可以解释为概率。\n",
    "- **缺点**：\n",
    "  - 容易出现梯度消失问题，尤其是在深层网络中。\n",
    "  - 输出值接近 0 或 1 时，梯度趋近于 0，导致训练缓慢。\n",
    "\n",
    "##### 2.2 Tanh 函数\n",
    "\n",
    "Tanh 函数的公式为：\n",
    "$$\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **特点**：\n",
    "  - 输出范围为 $(-1, 1)$。\n",
    "  - 是 Sigmoid 函数的平移和缩放版本。\n",
    "- **优点**：\n",
    "  - 输出值的均值为 0，有助于加速收敛。\n",
    "- **缺点**：\n",
    "  - 同样存在梯度消失问题。\n",
    "\n",
    "##### 2.3 ReLU 函数\n",
    "\n",
    "ReLU（Rectified Linear Unit）函数的公式为：\n",
    "$$\n",
    "\\text{ReLU}(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "- **特点**：\n",
    "  - 输出范围为 $[0, +\\infty)$。\n",
    "  - 是目前最常用的激活函数之一。\n",
    "- **优点**：\n",
    "  - 计算简单，收敛速度快。\n",
    "  - 缓解了梯度消失问题。\n",
    "- **缺点**：\n",
    "  - 当输入值为负时，梯度为 0，可能导致神经元“死亡”（Dead Neurons）。\n",
    "\n",
    "##### 2.4 Leaky ReLU 函数\n",
    "\n",
    "Leaky ReLU 函数的公式为：\n",
    "$$\n",
    "\\text{Leaky ReLU}(x) = \n",
    "\\begin{cases} \n",
    "x, & \\text{if } x > 0 \\\\\n",
    "\\alpha x, & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "其中，$\\alpha$ 是一个小的正数（通常为 0.01）。\n",
    "\n",
    "- **特点**：\n",
    "  - 输出范围为 $(-\\infty, +\\infty)$。\n",
    "- **优点**：\n",
    "  - 解决了 ReLU 的“死亡神经元”问题。\n",
    "- **缺点**：\n",
    "  - 需要手动设置 $\\alpha$。\n",
    "\n",
    "##### 2.5 Softmax 函数\n",
    "\n",
    "Softmax 函数的公式为：\n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}\n",
    "$$\n",
    "\n",
    "- **特点**：\n",
    "  - 输出范围为 $(0, 1)$，且所有输出值的和为 1。\n",
    "  - 常用于多分类任务的输出层。\n",
    "- **优点**：\n",
    "  - 将输出值转化为概率分布。\n",
    "- **缺点**：\n",
    "  - 对输入值的变化较为敏感。\n",
    "\n",
    "##### 2.6 Swish 函数\n",
    "\n",
    "Swish 函数的公式为：\n",
    "$$\n",
    "\\text{Swish}(x) = x \\cdot \\sigma(x) = x \\cdot \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "- **特点**：\n",
    "  - 是一种自适应的激活函数。\n",
    "- **优点**：\n",
    "  - 在某些任务中表现优于 ReLU。\n",
    "- **缺点**：\n",
    "  - 计算复杂度较高。\n",
    "\n",
    "---\n",
    "\n",
    "#### 3. TensorFlow 中的激活函数操作\n",
    "\n",
    "TensorFlow 提供了 `tf.keras.activations` 模块，其中包含了常用的激活函数。以下是一些示例：\n",
    "\n",
    "##### 3.1 使用内置激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a79b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 示例：使用内置的 ReLU 激活函数\n",
    "x = tf.constant([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "relu_output = tf.keras.activations.relu(x)\n",
    "print(\"ReLU 激活函数输出:\", relu_output.numpy())\n",
    "\n",
    "# 示例：使用内置的 Sigmoid 激活函数\n",
    "sigmoid_output = tf.keras.activations.sigmoid(x)\n",
    "print(\"Sigmoid 激活函数输出:\", sigmoid_output.numpy())\n",
    "\n",
    "# 示例：使用内置的 Tanh 激活函数\n",
    "tanh_output = tf.keras.activations.tanh(x)\n",
    "print(\"Tanh 激活函数输出:\", tanh_output.numpy())\n",
    "\n",
    "# 示例：使用内置的 Softmax 激活函数\n",
    "softmax_output = tf.keras.activations.softmax(x)\n",
    "print(\"Softmax 激活函数输出:\", softmax_output.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
