{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b8de58",
   "metadata": {},
   "source": [
    "## 数据限幅 上下限\n",
    "- clip_by_value\n",
    "- relu / max min\n",
    "- clip_by_norm\n",
    "- gradient clip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f35e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5890dc66",
   "metadata": {},
   "source": [
    "## tf.clip_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1654b97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "[[-10.   2.  30.]\n",
      " [  4. -50.  60.]]\n",
      "Clipped Tensor (by value):\n",
      "[[-5.  2. 10.]\n",
      " [ 4. -5. 10.]]\n"
     ]
    }
   ],
   "source": [
    "# 示例1：使用 tf.clip_by_value 限幅\n",
    "\n",
    "# 创建一个 2x3 的张量，其中包含一些超出上下限的值\n",
    "tensor = tf.constant([[-10.0, 2.0, 30.0], [4.0, -50.0, 60.0]])\n",
    "\n",
    "# 使用 tf.clip_by_value 对张量进行限幅操作\n",
    "# clip_value_min=-5.0 表示最小值限制为 -5.0\n",
    "# clip_value_max=10.0 表示最大值限制为 10.0\n",
    "clipped_tensor = tf.clip_by_value(tensor, clip_value_min=-5.0, clip_value_max=10.0)\n",
    "\n",
    "# 打印原始张量\n",
    "print(\"Original Tensor:\")  # 输出原始张量的值\n",
    "print(tensor.numpy())  # 使用 .numpy() 将张量转换为 NumPy 数组以便打印\n",
    "\n",
    "# 打印限幅后的张量\n",
    "print(\"Clipped Tensor (by value):\")  # 输出限幅后的张量的值\n",
    "print(clipped_tensor.numpy())  # 使用 .numpy() 将张量转换为 NumPy 数组以便打印"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2527d0",
   "metadata": {},
   "source": [
    "## 2 relu max min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e67089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor after applying tf.maximum (min value -5.0):\n",
      "[[-5.  2. 30.]\n",
      " [ 4. -5. 60.]]\n",
      "Tensor after applying tf.minimum (max value 10.0):\n",
      "[[-10.   2.  10.]\n",
      " [  4. -50.  10.]]\n"
     ]
    }
   ],
   "source": [
    "# 示例2：使用 tf.maximum 和 tf.minimum\n",
    "\n",
    "# 使用 tf.maximum 将张量中的每个元素与一个最小值进行比较，取较大的值\n",
    "min_value = -5.0\n",
    "max_tensor = tf.maximum(tensor, min_value)\n",
    "\n",
    "# 使用 tf.minimum 将张量中的每个元素与一个最大值进行比较，取较小的值\n",
    "max_value = 10.0\n",
    "min_tensor = tf.minimum(tensor, max_value)\n",
    "\n",
    "# 打印使用 tf.maximum 和 tf.minimum 操作后的张量\n",
    "print(\"Tensor after applying tf.maximum (min value -5.0):\")\n",
    "print(max_tensor.numpy())\n",
    "\n",
    "print(\"Tensor after applying tf.minimum (max value 10.0):\")\n",
    "print(min_tensor.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df48eaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor after applying tf.nn.relu:\n",
      "[[ 0.  2. 30.]\n",
      " [ 4.  0. 60.]]\n"
     ]
    }
   ],
   "source": [
    "# 示例3：使用 tf.nn.relu\n",
    "\n",
    "# 使用 tf.nn.relu 将张量中的负值替换为 0\n",
    "relu_tensor = tf.nn.relu(tensor)\n",
    "\n",
    "# 打印使用 tf.nn.relu 操作后的张量\n",
    "print(\"Tensor after applying tf.nn.relu:\")\n",
    "print(relu_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116b85b8",
   "metadata": {},
   "source": [
    "## tf.clip_by_norm\n",
    "\n",
    "`tf.clip_by_norm` 是 TensorFlow 中用于对张量进行范数限幅的方法。它通过将张量的 L2 范数限制在指定的最大值范围内，确保张量的值不会过大，从而避免梯度爆炸等问题。\n",
    "\n",
    "### 示例：使用 `tf.clip_by_norm` 对张量进行范数限幅\n",
    "\n",
    "以下是一个示例代码，展示如何使用 `tf.clip_by_norm`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b106b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "[[-10.   2.  30.]\n",
      " [  4. -50.  60.]]\n",
      "Clipped Tensor (by norm):\n",
      "[[-1.1851137   0.23702274  3.5553412 ]\n",
      " [ 0.4740455  -5.9255686   7.1106825 ]]\n"
     ]
    }
   ],
   "source": [
    "# 示例4：使用 tf.clip_by_norm\n",
    "\n",
    "# 创建一个 2x3 的张量\n",
    "tensor = tf.constant([[-10.0, 2.0, 30.0], [4.0, -50.0, 60.0]])\n",
    "\n",
    "# 使用 tf.clip_by_norm 对张量进行范数限幅\n",
    "# clip_norm=10.0 表示将张量的 L2 范数限制在 10.0 以内\n",
    "clipped_norm_tensor = tf.clip_by_norm(tensor, clip_norm=10.0)\n",
    "\n",
    "# 打印原始张量\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor.numpy())\n",
    "\n",
    "# 打印范数限幅后的张量\n",
    "print(\"Clipped Tensor (by norm):\")\n",
    "print(clipped_norm_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839fcc4c",
   "metadata": {},
   "source": [
    "### 代码解释\n",
    "\n",
    "1. **创建张量**：使用 `tf.constant` 创建一个包含多个值的张量。\n",
    "2. **范数限幅**：调用 `tf.clip_by_norm` 方法，将张量的 L2 范数限制在指定的最大值（如 10.0）以内。\n",
    "3. **打印结果**：\n",
    "  - 原始张量：显示未经过限幅操作的张量。\n",
    "  - 限幅后的张量：显示经过 `tf.clip_by_norm` 操作后的张量，其 L2 范数被限制在指定范围内。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21595afe",
   "metadata": {},
   "source": [
    "-----\n",
    "##  TensorFlow 中的梯度裁剪（Gradient Clipping）\n",
    "\n",
    "梯度裁剪是一种在深度学习中常用的技术，用于防止梯度爆炸问题。梯度爆炸通常发生在深度神经网络（尤其是 RNN）中，梯度的值在反向传播过程中变得非常大，导致模型无法收敛。\n",
    "\n",
    "TensorFlow 提供了多种梯度裁剪的方法，包括：\n",
    "1. **按值裁剪**：使用 `tf.clip_by_value` 将梯度限制在指定的最小值和最大值之间。\n",
    "2. **按范数裁剪**：使用 `tf.clip_by_norm` 将梯度的 L2 范数限制在指定范围内。\n",
    "3. **全局范数裁剪**：使用 `tf.clip_by_global_norm` 对所有梯度的全局范数进行裁剪。\n",
    "\n",
    "以下是一个使用梯度裁剪的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23ac9c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clipped Gradients:\n",
      "[[-8.79968181e-02  3.44668739e-02  1.86334714e-01 -1.01161242e-01\n",
      "  -5.92580251e-02  1.70760900e-01  3.88911983e-04  2.49473453e-02\n",
      "  -1.32606560e-02  1.68541029e-01]\n",
      " [ 1.01195471e-02  2.12006737e-02 -2.88916315e-04 -6.22245781e-02\n",
      "  -3.64497863e-02 -9.25172470e-04  4.69784689e-04  1.16571737e-02\n",
      "  -1.60181541e-02  5.02109490e-02]\n",
      " [-8.94954279e-02  3.75109203e-02  1.53087646e-01 -1.10095605e-01\n",
      "  -6.44915849e-02  2.36729577e-01  1.63215023e-04 -1.30668581e-02\n",
      "  -5.56511013e-03  1.72641158e-01]]\n",
      "[-0.10037925  0.09631145  0.3728942  -0.2826768  -0.16558586  0.28433126\n",
      " -0.00141477  0.01404836  0.04823916  0.3012443 ]\n",
      "[[-0.09686241]\n",
      " [-0.02893383]\n",
      " [-0.14626284]\n",
      " [-0.01541002]\n",
      " [-0.00344976]\n",
      " [-0.07881485]\n",
      " [ 0.04318966]\n",
      " [-0.0272631 ]\n",
      " [ 0.04280951]\n",
      " [-0.0688723 ]]\n",
      "[-0.5032773]\n"
     ]
    }
   ],
   "source": [
    "# 示例：使用 tf.clip_by_global_norm 进行梯度裁剪\n",
    "\n",
    "# 假设我们有一个简单的模型和损失函数\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(10, activation='relu'),\n",
    "  tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# 创建一个优化器\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# 定义一个简单的损失函数\n",
    "loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# 创建一些示例输入数据和标签\n",
    "x = tf.random.normal(shape=(5, 3))\n",
    "y = tf.random.normal(shape=(5, 1))\n",
    "\n",
    "# 使用 GradientTape 计算梯度\n",
    "with tf.GradientTape() as tape:\n",
    "  predictions = model(x)\n",
    "  loss = loss_fn(y, predictions)\n",
    "\n",
    "# 获取模型的可训练变量\n",
    "trainable_vars = model.trainable_variables\n",
    "\n",
    "# 计算梯度\n",
    "gradients = tape.gradient(loss, trainable_vars)\n",
    "\n",
    "# 对梯度进行全局范数裁剪\n",
    "clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=1.0)\n",
    "\n",
    "# 使用裁剪后的梯度更新模型参数\n",
    "optimizer.apply_gradients(zip(clipped_gradients, trainable_vars))\n",
    "\n",
    "# 打印裁剪后的梯度\n",
    "print(\"Clipped Gradients:\")\n",
    "for grad in clipped_gradients:\n",
    "  print(grad.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
